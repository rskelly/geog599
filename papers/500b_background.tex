\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{siunitx}
\usepackage{setspace}	% for line spacing
\usepackage{calc}		% for figure scaling
\usepackage{svg}		% for graphics
\usepackage{graphicx}	% for graphics
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{listings}

% Images are build by calling images/generate.sh <images> <output> where
% output is the "build" directory used by Texmaker.
\graphicspath{{./build/images/}}
\DeclareUnicodeCharacter{2010}{ }

\author{Rob Skelly}
\title{Real-Time Terrain Following and Trajectory Adjustment for Remote-Sensing Unmanned Aerial Vehicles}

\lstset{%
  basicstyle=\small\ttfamily,
  language=Python
}

\begin{document}

\maketitle

\doublespace


\section{Background}

\subsection{LiDAR}

Light detection and ranging (LiDAR) is a relatively new technology which uses laser pulses to measure the distance from the instrument to a surface. The typical LiDAR-equipped vehicle will use the global positioning system (GPS) for geodetic positioning, an inertial navigation system (INS) for measuring the inertial moments (attitude, angular velocity), a highly accurate clock, a computer and the laser instrument itself \cite{May2007,Lillesand1999}. LiDAR is considered an "active" sensor, as it provides its own source of energy, rather than relying on an environmental source, such as the sun. LiDAR works by sending out a pulse of laser light towards an object and recording the reflection of the pulse. The clock determines the time-of-return of the pulse, which then is used to calculate the distance that the pulse has travelled. Finally, the polar coordinate of the reflector is transformed to a three-dimensional coordinate in Cartesian space using the known position and orientation of the vehicle as determined by the GPS and INS. 

The laser itself may take many forms. Some LiDAR devices consist of a single, fixed laser while most consist of a single laser with a spinning or oscillating mirror that creates a linear scan pattern of repeated pulses. In the latter case, the computer has information about the orientation of the mirror and can accurately compute the position of the reflector based on this angle and the range. Recently, LiDAR devices have appeared for the autonomous vehicle market which feature multiple beams fixed to a spinning rotor. These differ from the spinning mirror type in that, with every revolution, they create multiple scan lines. Used in airborne applications, this enables re-sampling of the subject as the vehicle moves, and may result in a very high-density point cloud. The autonomous vehicle market places a premium on cost, simplicity and reliability, which has led to the proliferation of solid-state LiDAR devices. Unlike devices with rotating components, solid-state LiDARs have no moving parts and are much less likely to suffer a mechanical failure. These have the side-benefit that they are cheaper to manufacture and potentially much smaller and lighter than mechanical devices. Solid state LiDARs are available in single- and multi-beam configurations using multiple emitters, or a single emitter with a splitting lens.

The pulses emitted by the laser are not instantaneous, but consist of waves of light of stable intensity. The LiDAR's receiver reads the returning pulse as a waveform of time versus intensity. It discretizes this waveform and extracts the peaks as individual returns \cite{Lillesand1999}. This enables the device to extract more than one return from a single pulse. The "footprint" of a laser is not infinitely small, but tends to spread with distance. In complex environments, with multiple reflectors along the path of the laser, such as the leaves of a tree, the LiDAR may register returns from the top of the canopy, the ground and from intermediate obstructions. Some LiDAR devices record as many as five returns per pulse \cite{Lillesand1999}.

There are several measures of data quality for LiDAR data, including relative and absolute vertical and horizontal accuracy and point density. Positional accuracy may be degraded by numerous factors including platform position and attitude errors, boresight misalignment, range measurement error, scan angle error and beam divergence error \cite{May2007}, however these are somewhat beyond the scope of this project. Point density represents the number of points that can be found within a unit of continuous space, commonly described by a two- or three-dimensional grid. Because a LiDAR instrument emits a fixed number of pulses per unit time, and because the instrument-to-target distance is proportional to the coverage area, it is clear that point density at nadir declines with subject distance. 

As a LiDAR-equipped aircraft follows a flight line, a scanning LiDAR scans across the track. The pulses emitted by the instrument strike a flat surface at $90\degree$ at nadir, and at increasing oblique angles towards the ends of the scan line. The obliqueness of these angles has several consequences. First, a pulse striking a target at such an angle is likely to reflect away from the instrument, rather than back towards it; second, the increased distance that the pulse must travel increases the magnitude of the error inherent in the measurement (which is then inconsistent with the error at nadir); third, the pulse strikes the subject from the side, rather than the top, as intended. These issues are exacerbated when the platform's roll angle is non-zero. It is therefore common to filter out points within a certain distance of the flight line edge. The removal of edge points is compensated for, to some degree, by flying parallel lines with substantial overlap, up to 50\%. With this amount of overlap, every subject is effectively scanned twice. The consistency of this overlap is partially dependent on the consistency of the platform elevation and roll angle.

A further density-related issue is the role that platform pitch plays in the variability of point density. If the platform pitches forward, the scanner will rotate aft, creating a region of increased point density. However, if pitches rearward, it creates a region of reduced density. Clearly it is desirable to minimize variations in platform pitch. As some amount of pitch adjustment is necessary for gaining or losing altitude, this must be a consideration in the selection of a terrain-following strategy.

Point density has scale-determined implications for research. If a researcher is attempting to describe the canopy structure of $90\si{\m}$-tall old-growth forests \cite{Niemann2007,Niemann2012}, the platform elevation must be sufficient for the mission to encompass a significant number of crowns in a reasonable amount of time and to  minimize the variance in nominal point density between the ground and the tree top. The point density required for the study of such enormous plants is much lower than that required for a single grape vine, where a researcher may be interested in the orientation of individual leaves. To characterize the structure of a grape vine, point density must be suitably high, which, using the same instrument as used for the old growth, requires a lower flight elevation. However, if the vineyard is situated in a valley with considerable relief, the variance in point cloud density at a constant geodetic elevation will reduce the point density in lower-elevation sectors of the study area. For a given terrain, this effect is proportionally larger at lower nominal platform elevations.

It is important to note, here, the distinction between terrain elevation and surface elevation. A study using LiDAR may be concerned with the morphology of the terrain itself, or with objects on the terrain, such as plants and buildings. Whether the instrument's elevation follows the terrain, or the surface, which includes such objects, is decided by the purpose of the campaign. If the goal is to study objects on the terrain, it is usually desirable to maintain the platform's elevation with respect to the surface elevation, for example a forest canopy. If the subject of study is the terrain itself, the elevation should be maintained relative to it. The latter case introduces the problem of obstacle avoidance and object identification. For example, it must be determined whether an increase in terrain elevation is due to the appearance of a vegetative canopy or an actual up-slope in the terrain. If the former is true, and the platform is following the terrain elevation, it must be configured to avoid colliding with the trees. To some extent this is the responsibility of the pilot and mission planner, but the aircraft should possess the ability to protect itself from planning or piloting errors.

Point density and flight-line overlap are manageable, then, so long as the variation in platform elevation relative to the subject is minimized. There are two ways to accomplish this: first, by raising the platform to a sufficient elevation to minimize the variation in point density; second, by following the terrain at a constant relative elevation. This research is concerned with the second option.

\subsection{Hyperspectral Imaging}

A hyperspectral sensor, or imaging spectrometer, is a passive sensor that measures reflected light in very narrow frequency bands. Some such scanners can record over 400 distinct spectra. There are two primary types of hyperspectral sensor, the push-broom scanner, which is an array of sensor elements oriented across the scanning direction, and the whisk-broom scanner, in which an array of elements aligned with the scanning direction sweeps side-to-side across it \cite{Lillesand1999}.

The advantage of a hyperspectral scanner over a multi-spectral scanner, such as the Landsat Thematic Mapper, is the narrow width of its spectral bands. For example, band 7 of the Landsat TM instrument is $0.27\si{\um}$, while a typical hyperspectral scanner will record bands of $0.01\si{\um}$ in width \cite{Lillesand1999}. Just as a high spatial resolution reduces the mixing of spectra in a pixel, a high spectral resolution reduces mixing within spectra. If, for example, there is an absorption feature somewhere within Landsat's band 7, it will be impossible to discern. A collection of narrower  bands across the same span of wavelengths will be able to resolve the feature.

A major issue with all spectrometers is the interaction between light and the atmosphere. The ultimate goal of spectrometry is to determine the reflectance of a surface, which is presumed to correlate with some characteristic of or process within the underlying object. In a vacuum, reflectance is simply the ratio between irradiance (energy from the source) and radiance (energy reflected from the subject). However, under natural conditions, the spectrometer measures radiance from the subject plus energy reflected from the atmosphere between the sensor and the target (path radiance). Irradiance is then the sum of attenuated light sources, generally the sun and incident radiation from the atmosphere \cite{Lillesand1999}. Unfortunately, these atmospheric effects cannot be ignored, even at the low elevations flown by UAVs. However, the variations can be minimized by maintaining a constant elevation and thus a consistent volume of atmosphere between the sensor and the subject.

Platform elevation has several effects on hyperspectral data quality aside from atmospheric effects. The first of these is the effect on resolution, as discussed above: as the platform elevation increases, the sensor's spatial resolution -- that is, its ability to resolve discrete objects -- declines, resulting in spectral mixing within pixels. Another effect is on the ratio of signal to noise. Each element of a hyperspectral scanner captures a discrete value by recording incoming radiation for a specified interval, called the integration time. For a given platform velocity and integration time (which are set during the planning phase of the mission), the amount of energy received by an element is proportional to the area of visible reflective surface, which is in turn proportional to the platform's elevation. Since the instrument noise is constant, the reduced proportion of incoming energy reduces the signal-to-noise ratio. In order to maintain this ratio despite variations in elevation, the platform must either adjust its speed, which has other undesirable consequences, or the sensor's integration time, which is not possible or desirable in real time. The integration time has consequences with respect to vehicle velocity as well. When the sensor is active, it smears across the target for the duration of its integration time. So long as the velocity is constant and pitching movements are minimized, this effect is constant.

\subsection{Terrain Following}

The quality issues highlighted in previous sections can be addressed, to an extent, through careful control of the platform's elevation, which should remain constant relative to the surface of interest. The axial movements of the platform should also be minimized. However, unlike a fixed-wing aircraft and helicopters, which maintain the attitudes of their chassis using control surfaces and tilting rotors, respectively, a multi-rotor UAV moves by coordinating the differential thrust of its fixed rotors to rotate its chassis. This poses a unique challenge for the autonomous control of UAVs used in remote sensing.

When a UAV encounters a change in surface elevation it must adjust the vertical component of its trajectory. Due to the inertia of the platform, the change in velocity is necessarily delayed until some time after the vehicle's thrust and attitude have been adjusted. It is thus impossible for a vehicle to follow a terrain exactly using a purely reactive system without backtracking. This situation pertains when the vehicle's rangefinder is oriented to nadir; if the vehicle approaches a vertical face and is not capable of sensing forward, it will contact the face.  Alternatively, if the vehicle has the ability to sense changes in surface elevation some distance ahead in the direction of travel, should it encounter a vertical face, it can take evasive action.

In a naive approach to the problem, at the appearance of a vertical obstacle, the vehicle's horizontal velocity must drop to zero and its vertical velocity must increase from zero. This is undesirable in a remote sensing context as, during the deceleration, the sampling density will increase until, during the ascent, the instruments will continue to sample the same ground repeatedly. After the ascent, there will be a brief flight over the target surface while the vehicle is at the new elevation. Scale distortion in this region will be maximized. Also at this time, the vehicle will begin accelerating in the horizontal direction again, with gradually-decreasing over-sampling over the elevated surface. These anomalies will have to be rectified in post-processing, if possible. This strategy also entails a considerable increase in flight time as, during the climb, the vehicle is not advancing along its flight plan, but is dedicating considerable power resources to hovering and climbing. Alternatively, the vehicle can follow a smoothed trajectory which departs from the nominal flight elevation but preserves the vehicle's horizontal velocity (figure \ref{fig:uav_smooth_traj}). In this case, scale distortion is induced in the data, but the along-track sampling density remains constant.

There are mechanisms for enabling terrain following for UAVs which depend on the pre-existence of a digital surface model (DSM). This has two consequences: first, that the survey must have been performed previously and the resulting data processed into a DSM; second, that the DSM must be of sufficient quality, accuracy and detail to control the UAV safely and accurately enough to satisfy the data-quality demands of a remote sensing campaign, given the nature of the terrain, ground cover and the desired flight elevation. Currently-available solutions \cite{ArduPilot2017,Tudor2017} use the Shuttle Radar Topography Mission (STRM) elevation model, which is one of few freely-accessible global digital elevation models (DEMs), but is not a true DSM; C-band RADAR may penetrate up to half way through a forest canopy \cite{Carabajal2005} and will penetrate into snow, depending on its porosity and wetness \cite{Tighe2009}. The maximum resolution of the SRTM DEM is $30'$ (approximately $30\si{\m}$) horizontally and $1\si{\m}$ vertically, with stated vertical accuracy of $\leq16\si{\m}$, much too coarse for low-elevation remote sensing work, particularly in steep or forested terrain \cite{Smith2003a,Tom2008}. An ideal solution would be for the UAV to compute a surface model on its own, in real time, and follow the surface at a fixed elevation while executing its flight plan. 

\begin{figure}
\centering
\def\svgscale{0.5}
\input{build/images/uav_smooth_trajectory.pdf_tex}
\caption{Zero-velocity and smoothed trajectory adjustments. Black arrows indicate regions of scale distortion. Dashed arrows indicate regions of over-sampling due to changes in horizontal velocity.}
\label{fig:uav_smooth_traj}
\end{figure}

The problem of platform stability during elevation adjustments is complex. A hovering UAV requires an amount of thrust (measured in Newtons, $N$) equal to the downward force on the vehicle, given by

\begin{equation}
f = ma,
\label{eq:hover_force}
\end{equation} where $f$ is the downward force, also in Newtons, $m$ is the mass of the vehicle in kg, and $a$ is the acceleration of gravity on Earth, approximately $9.8m/s^2$. A UAV moving forward at a steady velocity requires an amount of thrust equal to the hovering thrust plus an additional force to maintain altitude while it moves horizontally. This is given by,
 
\begin{equation}
v_i = \dfrac{ v_h^2 } { \sqrt{ (v_\infty \cos \alpha)^2 + (v_\infty \sin \alpha + v_i)^2 } } ,
\label{eq:move_force}
\end{equation} where $\alpha$ is the angle of attack, $v_\infty$ is the free stream speed (platform velocity plus wind) and $v_h$ is the hover velocity \cite{Hoffmann2007}. If the maximum available thrust of the platform is known, this provides an approximate limit on the rate of climb achievable by the vehicle, which, in turn, places limits on the amount of advance warning required by the vehicle for a given change in elevation. 

With a forward-facing rangefinder, the problem of an approaching obstacle is easily resolved, but descending presents another problem. As the vehicle approaches the lip of a cliff, the terrain below the face is occluded by its edge (figure \ref{fig:uav_edge_occlude}) depriving the vehicle of elevation information that it needs to optimize its trajectory. With a single-beam rangefinder, at one time step, the lip will be sampled, and at the next, the ground will be sampled. The vehicle can estimate the terrain as the convex hull of these elevations and derive a trajectory based on this model, but it cannot revise this estimate or its trajectory without a rangefinder with a broader vertical field of view, or one or more additional rangefinders directed downward at a steeper angle. A similar phenomenon occurs when the vehicle is traversing down a slope that does not intersect with the rangefinder's beam. If the end of the slope is beyond the instrument's range, the vehicle loses contact with the ground and has no information from which to compute a trajectory.  Ideally, the vehicle could have a sensor that is sensitive from nadir to the horizon, and adjust its trajectory iteratively as it approaches the edge. 

\begin{figure}
\centering
\def\svgscale{0.8}
\input{build/images/uav_edge_occlude.pdf_tex}
\caption{Rangefinder occlusion on obstacle edge.}
\label{fig:uav_edge_occlude}
\end{figure}

To identify a sufficient advance angle for a forward-facing rangefinder, it is necessary to first calculate the time required for a UAV to change its velocity. At the extreme, in the case of the vertical cliff of indeterminate height, this is the time required to reduce the vehicle's horizontal velocity to zero. It would be a simple matter to calculate the time required for the vehicle to decelerate given its mass, available thrust, velocity and environmental conditions. However, owing to the fact that a multi-copter must control its speed by pitching, which should be minimized to protect data quality, the rate of deceleration should be carefully considered. On the other hand, if the horizontal distance dedicated to ascending is to be minimized (to minimize the horizontal distance affected by scale distortion), the vehicle's maximum thrust must be considered -- it must be able to ascend rapidly enough to clear the terrain without losing horizontal velocity.

Research into terrain following has been continuing since the 1960s for the control of ballistic missiles and jet aircraft \cite{KRACHMALNICK1968,Starling1971,Cunningham1980}. Such systems use a forward- and downward-facing RADAR rangefinder to detect the terrain elevation some distance ahead of the vehicle, some means of terrain classification \cite{Cunningham1980}, and a method of altering the vehicle's trajectory to follow the terrain surface as closely as possible. Research on terrain following for military aircraft and missiles has been focused on high-speed, low elevation flight with a premium placed on stealth, speed, efficiency and safety \cite{KRACHMALNICK1968}. It is important to note the distinction between terrain following and terrain avoidance, here: terrain avoidance involves changing the trajectory of the vehicle, vertically or horizontally, in order to avoid terrain; terrain following implies the maintenance of a minimum distance to the terrain, but also a maximum distance. Missiles and military aircraft must fly low to avoid detection by radar and interception by surface-to-air or airborne weapons. Neither high speed nor stealth are desirable for a remote sensing mission but many of the insights from this research are nevertheless applicable to remote-sensing UAVs.

The key feature of these terrain following systems is their predictive, rather than reactive, nature. Most existing UAV terrain following systems have used either a nadir-aligned rangefinder and a feedback loop for adjusting the vehicle's elevation to a predetermined value in real time \cite{ArduPilot2017}, or a pre-exiting elevation model which is interpolated to generate a three dimensional flight path \cite{ArduPilot2017,Tudor2017,Samar2011}. Military terrain following systems provide both the ability to adaptively respond to terrain without pre-planning, and the ability to optimize their flight plans in flight with some degree of foresight. 

For example, Starling \cite{Starling1971} envisioned a RADAR-based system featuring a virtual "ski" sliding on the terrain ahead of, and in alignment with, the aircraft. As the ski encountered a rise, its axis would intersect that of the aircraft, generating a climb-command, causing the aircraft to climb. As the ski rode the terrain down again, the axes of the aircraft and the ski would diverge, generating a descend-command. This feature had the valuable quality that on low-frequency terrain, the aircraft would follow it closely. On high-frequency terrain, the aircraft would tend to smooth the terrain, following a trajectory that would not exceed the gravitational tolerances of the pilot (about 1g). Several authors \cite{MENON1991,Popovic2017,Lu1995,Rahim2011,Samar2011} suggest the use of polynomial splines for developing smoothed trajectories from terrain elevations, and Lu and Pierson \cite{Lu1995} documented a method for terrain-following in a fixed-wing aircraft by exploiting the lift and drag properties of the vehicle and "bang-bang" (on/off) throttle control. 

None of these strategies are ideal for remote sensing UAVs. In the first and second instances, maintaining a constant elevation with respect to terrain immediately below the vehicle is not the researchers' primary concern. In the third, reliance on the aerodynamic lift of the craft to smooth elevation changes in a power-off situation is a non-starter, as multi-copters have essentially none.

Alqahtani and Emron \cite{Alqahtani2018} devised a system using using downward- and forward-facing rangefinders with a Gaussian surface filter with various window sizes to anticipate terrain undulations and project a forward trajectory. This research generated some important insights and resolved some of the issues highlighted at the beginning of this section. The authors found that, though a small Gaussian window would enable the vehicle to follow terrain closely, it would also cause the vehicle to hit large obstacles if the laser were not aimed high enough. The occlusion problem (figure \ref{fig:uav_edge_occlude}) could be resolved, in part, by reducing the Gaussian window dynamically according to the orientation of the terrain surface. The authors found that this strategy could reduce the vehicle's power consumption by eliminating hard transitions in and out of the altitude control loop.

The use of a three-dimensional point cloud and ground classification algorithms do not seem to have appeared in the literature on UAV terrain following, perhaps because LiDAR devices small enough for use on such vehicles are a recent innovation. While Alqahtani and Emron \cite{Alqahtani2018} did not address the use these devices, their work nevertheless provides a valuable starting point for research in this area. 

Because a multi-copter is entirely dependent on thrust, rather than lift, for the control of its elevation, it is relatively easy to calculate the amount of power required for a given vertical trajectory. Power consumption is proportional to thrust, which is, in turn, directly determined by the mass and dynamic state of the vehicle. This implies that the power curve can be calculated -- at least naively -- from the trajectory. Total power consumption for the manoeuvre is then given by the integral of the power curve \cite{Halliday_Resnick_1988}.

\begin{figure}
\centering
\def\svgscale{0.8}
\input{build/images/uav_power_curve.pdf_tex}
\caption{The estimated power curves resulting from two different responses to a terrain event.}
\label{fig:uav_power_curve}
\end{figure}


\subsection{Rangefinder and Algorithm Selection}

The success of a surface-following application depends on the ability to accurately locate the terrain with respect to a moving vehicle. There are many instruments capable of providing this service, including RGB cameras, lasers, sonar devices and RADAR. Laser and camera rangefinders are considered here.

\subsubsection{RGB Camera}

A digital RGB camera with a wide-angle lens will cover a wide field of view at high resolution and reasonably low cost relative to other instruments, but will require computationally-intensive photogrammetry or optical-flow processing to extract range measurements \cite{Campos2016,Herisse2010,Netter2002,Hammoud2011}, though, as noted by Campos \cite{Campos2016}, the most processing-intensive step in this process may be the image encoding. In general, the quality and resolution of a camera are proportional to its cost and weight. Also, spectrometers are passive sensors, meaning they record reflected light from variable environmental sources, specifically the sun, and are thus subject to weather conditions and the availability of daylight. Tracking changes in intensity or colour across the pixel array becomes difficult without the consistency of a controlled energy source or on low-contrast and complex surfaces, hampering the detection of objects which would be used to compute ranges. 

Optical flow is a method that attempts to estimate the structure of a three-dimensional space by tracking the changes in position of objects within the space, as projected into two-dimensional space (the image itself). The motion of objects is represented in the image as changes in intensity of pixels within the pixel array. This method works only when the camera motion, hence platform velocity, is sufficient to induce detectable changes in the pixel array during the required time interval, and may fail when the magnitude of change exceeds a reference value \cite{Srinivasan1994}. Optical flow works best over planar terrain, with errors of up to 10\% \cite{Campos2016} on more complex surfaces. Also, platform rotation can interfere with the accuracy of range measurements and the penetration of porous surfaces, such as forest canopies is not possible. Indeed, such surfaces may confuse the algorithm. Interestingly, in studies of photo-flow for use in UAV terrain following, single-beam laser rangefinders mounted with the camera are often used for validation \cite{Campos2016}.

Photogrammetry is a method using parallax to generate a point cloud from sequential image frames \cite{Carrivick2016}. These points may then be used as if they were generated by a scanning LiDAR, however this method suffers from many of the same limitations as optical flow.

\subsubsection{Single-beam Laser Rangefinders}

One or more single-beam laser range finders can be mounted statically \cite{Alqahtani2018}, on a scanning gimbal \cite{LightWare2017} or in a mixture of configurations. These instruments use the time-of-return of a pulse of laser light to calculate the distance to a reflector. This distance must be transformed, if desired, into a polar or Cartesian coordinate by an external computer. Laser rangefinders are active scanners that emit their own energy, so they are less dependent on environmental conditions and scene contrast for measurement accuracy, however measurement density with respect to time may be reduced by scattering, absorption or reflection on some surfaces, such as water which both reflects and absorbs light at the typical wavelength of $\sim1000\si{\nm}$. These devices are cheap and light but do not offer the spatial coverage of either a digital camera or a scanning LiDAR, making the intelligent analysis of terrain conditions an   d ground cover -- as opposed to the simple elevation -- difficult. However, they are highly accurate within their nominal range \cite{Campos2016} and the type and volume of data they produce is fairly trivial to process using a Gaussian \cite{Alqahtani2018}, Kalman \cite{Kalman1960} or other smoothing regime. High-quality versions of these instruments produce over 18,000 pulses per second at accuracies of $\pm10\si{\cm}$ or better \cite{LightWare2016}.


\subsubsection{Scanning LiDAR}

Scanning LiDAR devices -- whether spinning or oscillating mirror type, rotating single- or multi-beam or solid state -- provide scene coverage second only to that of a digital camera and, being active sensors, suffer less from the influence of environmental conditions. These are heavier than other options, consume more power, and produce a large enough volume of data that processing can be complex and time consuming, but they resolve most of the problems inherent in digital cameras and single-beam rangefinders. With a high-enough scanning frequency, the servo-mounted single-beam laser could be included in this category.

With the advent of automotive multi-beam scanning LiDAR devices, a unique opportunity arises. Unlike traditional scanning LiDARs, these devices produce multiple, simultaneous scan-lines over an arc from $30\degree$ to $40\degree$. By tilting these instruments slightly forward, they can serve as both the main remote sensing instrument, and the the terrain following sensor, and can provide sufficient point density and coverage for extremely accurate real-time terrain characterization. For a vehicle flying at 35m elevation, a LiDAR scanning $30\degree$ forward offers approximately $20\si{\m}$ of advance coverage, or $4\si{\s}$ at $5\si{\m\per\s}$. 

With the dense, three-dimensional point clouds produced by scanning LiDARs, there arises the opportunity to characterize terrain features in a way that is impossible with other methods. This is due in part to the distribution of points and to their number. If the space under examination is discretized into usable parcels, there are enough measurements in each to perform a robust statistical analysis and classification of the features that lie therein. There are many methods for analyzing dense point clouds, removing noise and classifying objects, but it is important to remember for this research that the object is less the accurate classification or analysis of ground cover and more the rapid determination of a safe and accurate trajectory.

A Cartesian LiDAR-derived z-coordinate is composed of three elements, 

\begin{equation}
E_{sensor} = E_{ground} + E_{non-ground} + M_{noise},
\label{eq:decel}
\end{equation} 

where $E_{sensor}$ is the measured height, $E_{ground}$ is the true ground elevation, $E_{non-ground}$ is the height of objects on the ground, and $M_{noise}$ is measurement error, which may arise from a variety of sources \cite{Meng2010}. The various classification methods interact with different parts of this triad, with different goals. Isolating the terrain elevation may be a simple case of marking all last-return points points as ground, and then removing outliers according to specified criteria. Though objects on the terrain are often penetrable by the laser and therefore distinguishable from terrain, the opposite is sometimes true. When such objects are opaque, a more sophisticated method must be used to isolate and identify them.

Motivated by Tobler's principle, in which objects nearer to each other in space are likely to be more similar, Vosselman \cite{Vosselman2000} proposed a method whereby the slope measured between two adjacent points could determine, based on a threshold, whether both were ground, or one was non-ground. In this case, the threshold angle would be determined by the scale of measurement and the nature of the environment. For example, a very low threshold angle would detect a person standing on a football field, while it would take a significantly higher angle to detect the same person on a steep slope. Also in the Toblerian vein, Khosravipour \cite{Khosravipour2016} used the "spikiness" of facets in a triangulated irregular network (TIN) of LiDAR points to identify points that did not belong to the terrain surface; given the presumption of regularity in the distribution of points in the horizontal dimensions, an overly acute angle in any facet could likely be caused by an extreme in the vertical dimension. 

The progressive morphological filter \cite{Zhang2003} uses a progressively-opening "window," within which the minimum-elevation point is selected, to iteratively distinguish ground points from objects. Chang \emph{et al} \cite{Chang2008} used the detection of occluded -- that is, empty -- regions in an oblique scan to identify and filter non-ground points, and there are other methods using thin plate splines \cite{Hudak2012}, Hermite splines \cite{Silvan-Cardenas2006} and others \cite{Hodgson2005,Zheng2007,Zhang2005}.

These methods have in common single object of accuracy with respect to the true terrain elevation and the identification of points that are representative of terrain. For the purposes of terrain following, accuracy and object identification are less important than efficiency and safety. To determine the correct flight elevation over terrain, the UAV needs a reasonably accurate representation of the elevation, excluding objects such as plants and vehicles, for example, but the representation will be smoothed as it is developed into a trajectory so the level of precision required scientific work is not required. For vehicles that must follow the surface, which includes the vegetative canopy or buildings, the vehicle need not perform any classification at all, only remove points suspected of being noise and develop a smooth trajectory over the remainder. This considerably simplifies the task of developing the surface model.

One possible solution, simplified for the purposes of navigation, is to store all LiDAR points during a time interval into a grid of lists, with each list representing a vertical column with a predetermined horizontal spatial extent. At the end of the interval, the mean of a selected quantile will be used as the elevation -- a high quantile for a forest canopy, for example, and a low quantile for terrain.


\subsection{Mission Planning}

For remote sensing surveys using UAVs, missions are generally not flown manually by the pilot, but planned using software that lays out a regular grid of flight lines optimized for data resolution and flight line overlap. However, these mission plans are two-dimensional only, or require the use of a DEM, and must be augmented by an auxiliary terrain-following process. Two such software products are Maps Made Easy \cite{Tudor2017} and ArduPilot \cite{ArduPilot2017}, which produce flight plans for popular UAVs made by the DJI company.


\bibliographystyle{plain}
\bibliography{/home/rob/Documents/bibtex/library.bib}


\end{document}